{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural-based propaganda detection\n",
    "Propaganda is the new weapon that influences people's opinions or beliefs about a certain ideology, whether that ideology is right or wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. loading Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "ExecuteTime": {
     "end_time": "2024-12-09T07:46:42.170523Z",
     "start_time": "2024-12-09T07:46:42.137131Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "df = pd.read_table('train.tsv')\n",
    "df = shuffle(df) # randomly shuffle data entries\n",
    "df"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      article_id                                      article_title  \\\n",
       "9214   765953146                      'Textbook Definition of Bias'   \n",
       "5963   764609985  Clinton Email IG Report Rips FBI, Comey, & Lyn...   \n",
       "9925   705409419  ﻿Vatican Theologian Sacked for Questioning “Me...   \n",
       "1825   756114837  WHO Prepares For “Worst Case” As Congo Ebola O...   \n",
       "9078   755814432   Trump’s Plan for Iran: Put Terrorists in Charge?   \n",
       "...          ...                                                ...   \n",
       "3895   706600938            Did Saint Francis Predict Pope Francis?   \n",
       "5336   728972961  FOR THE FIRST TIME ONLINE: Archbishop Lefebvre...   \n",
       "11     775448623          3-D-printed guns put carnage a click away   \n",
       "3969   696264594  The American Jewish Historical Society Hosts D...   \n",
       "161    769962236    Communist Door Boy Attacks Teen Trump Supporter   \n",
       "\n",
       "               label                                      sentence_text  \n",
       "9214  non-propaganda  On Tuesday Horowitz performed solo before a jo...  \n",
       "5963  non-propaganda  “[Trump’s] not ever going to become president,...  \n",
       "9925  non-propaganda  Weinandy’s very point about fearfulness and la...  \n",
       "1825  non-propaganda                                        -Daily Mail  \n",
       "9078  non-propaganda  Whom do you consider to be the most corrupt De...  \n",
       "...              ...                                                ...  \n",
       "3895      propaganda  But he had no evidence that the quotation as s...  \n",
       "5336  non-propaganda                               But still they come.  \n",
       "11        propaganda  So it was stunning — but not surprising, given...  \n",
       "3969  non-propaganda  The American Jewish Historical Society was fou...  \n",
       "161   non-propaganda  “You may not like the hat or you may not like ...  \n",
       "\n",
       "[11464 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>article_title</th>\n",
       "      <th>label</th>\n",
       "      <th>sentence_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9214</th>\n",
       "      <td>765953146</td>\n",
       "      <td>'Textbook Definition of Bias'</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>On Tuesday Horowitz performed solo before a jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5963</th>\n",
       "      <td>764609985</td>\n",
       "      <td>Clinton Email IG Report Rips FBI, Comey, &amp; Lyn...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>“[Trump’s] not ever going to become president,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9925</th>\n",
       "      <td>705409419</td>\n",
       "      <td>﻿Vatican Theologian Sacked for Questioning “Me...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>Weinandy’s very point about fearfulness and la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>756114837</td>\n",
       "      <td>WHO Prepares For “Worst Case” As Congo Ebola O...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>-Daily Mail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9078</th>\n",
       "      <td>755814432</td>\n",
       "      <td>Trump’s Plan for Iran: Put Terrorists in Charge?</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>Whom do you consider to be the most corrupt De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3895</th>\n",
       "      <td>706600938</td>\n",
       "      <td>Did Saint Francis Predict Pope Francis?</td>\n",
       "      <td>propaganda</td>\n",
       "      <td>But he had no evidence that the quotation as s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5336</th>\n",
       "      <td>728972961</td>\n",
       "      <td>FOR THE FIRST TIME ONLINE: Archbishop Lefebvre...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>But still they come.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>775448623</td>\n",
       "      <td>3-D-printed guns put carnage a click away</td>\n",
       "      <td>propaganda</td>\n",
       "      <td>So it was stunning — but not surprising, given...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3969</th>\n",
       "      <td>696264594</td>\n",
       "      <td>The American Jewish Historical Society Hosts D...</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>The American Jewish Historical Society was fou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>769962236</td>\n",
       "      <td>Communist Door Boy Attacks Teen Trump Supporter</td>\n",
       "      <td>non-propaganda</td>\n",
       "      <td>“You may not like the hat or you may not like ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11464 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Pre - Analysis on  data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "ExecuteTime": {
     "end_time": "2024-12-09T07:46:42.228264Z",
     "start_time": "2024-12-09T07:46:42.223978Z"
    }
   },
   "source": [
    "raw_labels = df.label.values.tolist()\n",
    "docs = df.sentence_text.values.tolist()\n",
    "titles = df.article_title.values.tolist()\n",
    "\n",
    "label_dic = {'non-propaganda':0, 'propaganda':1}\n",
    "\n",
    "assert len(docs) == len(raw_labels) == len(titles)\n",
    "labels = [label_dic[rl] for rl in raw_labels] # transfer raw labels (strings) to integer numbers\n",
    "print('total data size: {}, label type num: {}'.format(len(docs), len(label_dic)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data size: 11464, label type num: 2\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "ExecuteTime": {
     "end_time": "2024-12-09T07:46:42.270267Z",
     "start_time": "2024-12-09T07:46:42.267737Z"
    }
   },
   "source": [
    "# take a look at some sentences in the dataset\n",
    "print(docs[19])\n",
    "print(titles[19])\n",
    "print(labels[19])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For some, it seems, no price is too great—not even the good of the Church—to avoid the personal ignominy of being seen on the wrong side of history.\n",
      "Archbishop Viganò Speaks, the Neo-Catholics Panic\n",
      "0\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "ExecuteTime": {
     "end_time": "2024-12-09T07:46:42.294975Z",
     "start_time": "2024-12-09T07:46:42.290691Z"
    }
   },
   "source": [
    "# split the data into train, dev and test\n",
    "\n",
    "train_ratio, dev_ratio, test_ratio = 0.6, 0.2, 0.2\n",
    "train_docs = docs[:int(len(docs)*train_ratio)]\n",
    "train_labels = labels[:int(len(docs)*train_ratio)]\n",
    "\n",
    "dev_docs = docs[int(len(docs)*train_ratio):int(len(docs)*(train_ratio+dev_ratio))]\n",
    "dev_labels = labels[int(len(docs)*train_ratio):int(len(docs)*(train_ratio+dev_ratio))]\n",
    "\n",
    "test_docs = docs[-int(len(docs)*(test_ratio)):]\n",
    "test_labels = labels[-int(len(docs)*(test_ratio)):]\n",
    "\n",
    "print('train size {}, dev size {}, test size {}'.format(len(train_labels), len(dev_labels), len(test_labels)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size 6878, dev size 2293, test size 2292\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. Loading the Glove Embedding\n",
    "#### The Glove embedding considers every words as a single entity and creates a vector for each word. So i prefer to stick with Glove."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "ExecuteTime": {
     "end_time": "2024-12-09T07:48:35.968198Z",
     "start_time": "2024-12-09T07:46:42.323743Z"
    }
   },
   "source": [
    "# load the glove pre-trained embedding\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "path_of_downloaded_files = \"/MLOps/2.Propaganda_detection/glove.6B.300d.txt\"\n",
    "glove_file = datapath(path_of_downloaded_files)\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.300d.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "word_vectors = KeyedVectors.load_word2vec_format(word2vec_glove_file)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v_/00jh03xj7ln1tm7xb4vvnwg40000gn/T/ipykernel_1137/3888605542.py:9: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_file, word2vec_glove_file)\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5. Vectorize (OOV) Words"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "ExecuteTime": {
     "end_time": "2024-12-09T07:48:36.031101Z",
     "start_time": "2024-12-09T07:48:36.027507Z"
    }
   },
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "word_vec_dim = 300                    # make sure this number matches the embedding you use\n",
    "oov_vec = np.random.rand(word_vec_dim) \n",
    "def vectorize_sent(word_vectors, sent):\n",
    "    word_vecs = []\n",
    "    for token in word_tokenize(sent): \n",
    "        if token not in word_vectors: \n",
    "            word_vecs.append(oov_vec)\n",
    "        else:\n",
    "            word_vecs.append(word_vectors[token].astype('float64'))\n",
    "    return np.mean(word_vecs,axis=0)\n",
    "\n",
    "vv = vectorize_sent(word_vectors, 'hello world ! this is a test sentence !')\n"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "source": [
    "# create vector representations; \n",
    "# TODO: consider to apply necessary text cleaning/normalization techniques\n",
    "# TODO: consider whether to use titles information (the example below does not use titles but only sentences)\n",
    "\n",
    "\n",
    "train_vecs = np.array([vectorize_sent(word_vectors, ss) for ss in train_docs])\n",
    "dev_vecs = np.array([vectorize_sent(word_vectors, ss) for ss in dev_docs])\n",
    "\n",
    "\n",
    "print(train_vecs.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "ExecuteTime": {
     "end_time": "2024-12-09T07:48:37.378396Z",
     "start_time": "2024-12-09T07:48:36.036571Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6878, 300)\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "source": [
    "# define a simple MLP (multi-layer perceptron) as the classifation model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, out_dim, dp_rate):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_layer = nn.Linear(input_dim, input_dim*2)\n",
    "        self.output_layer = nn.Linear(input_dim*2, out_dim)\n",
    "        self.dropout = nn.Dropout(dp_rate)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "       \n",
    "    def forward(self, x_in):\n",
    "        z1 = self.dropout(x_in) # output of the input layer, after dropout\n",
    "        z2 = self.relu(self.hidden_layer(z1)) # output of the hidden layer\n",
    "        logits = self.output_layer(z2)\n",
    "        return logits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "ExecuteTime": {
     "end_time": "2024-12-09T07:48:37.408253Z",
     "start_time": "2024-12-09T07:48:37.404740Z"
    }
   },
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "source": [
    "# build model\n",
    "dropout_rate = 0.5 \n",
    "model = MLP(word_vec_dim,len(label_dic),dropout_rate) \n",
    "loss_fnc = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# hyper parameters\n",
    "n_epochs = 50 # number of epoch (i.e. number of iterations)\n",
    "batch_size = 32 # mini batch size\n",
    "lr = 0.001 # initial learning rate\n",
    "\n",
    "# initialize optimizer and scheduler (lr adjustor)\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=lr) # use Adam as the optimizer\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.9) # decays the learning rate of each parameter group by gamma every step_size epochs."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "ExecuteTime": {
     "end_time": "2024-12-09T07:48:37.440955Z",
     "start_time": "2024-12-09T07:48:37.436342Z"
    }
   },
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "ExecuteTime": {
     "end_time": "2024-12-09T07:48:47.070319Z",
     "start_time": "2024-12-09T07:48:37.467617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "best_f1 = -1.\n",
    "best_model = None\n",
    "import copy\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "for epoch_i in range(n_epochs):\n",
    "    # the inner loop is over the batches in the dataset\n",
    "    model.train() # let pytorch know that gradients should be computed, so as to update the model\n",
    "    for idx in range(0,len(train_vecs),batch_size):\n",
    "        # Step 0: Get the data\n",
    "        x_data = torch.tensor(train_vecs[idx:idx+batch_size], dtype=torch.float)\n",
    "        if x_data.shape[0] == 0: continue\n",
    "        y_target = torch.tensor(train_labels[idx:idx+batch_size], dtype=torch.int64)\n",
    "\n",
    "        # Step 1: Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Step 2: Compute the forward pass of the model\n",
    "        y_pred = model(x_data)\n",
    "\n",
    "        # Step 3: Compute the loss value that we wish to optimize\n",
    "        loss = loss_fnc(y_pred, y_target)\n",
    "\n",
    "        # Step 4: Propagate the loss signal backward\n",
    "        loss.backward()\n",
    "\n",
    "        # Step 5: Trigger the optimizer to perform one update\n",
    "        optimizer.step()\n",
    "\n",
    "    # after each epoch, we can test the model's performance on the dev set\n",
    "    with torch.no_grad(): # let pytorch know that no gradient should be computed\n",
    "        model.eval() # let the model know that it in test mode, i.e. no gradient and no dropout\n",
    "        dev_data = torch.tensor(dev_vecs, dtype=torch.float)\n",
    "        dev_target = torch.tensor(dev_labels, dtype=torch.int64)\n",
    "        dev_prediction = model(dev_data)\n",
    "        pred_labels = [np.argmax(dp.numpy()) for dp in dev_prediction]\n",
    "        pre, rec, f1, _ = precision_recall_fscore_support(dev_target, pred_labels, average='macro')\n",
    "        print('\\n---> after epoch {} the macro-f1 on dev set is {}'.format(epoch_i, f1))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print('learning rate', param_group['lr'])\n",
    "\n",
    "        # save the best model\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            print('best model updated; new best f1',f1)\n",
    "\n",
    "    # (optional) adjust learning rate according to the scheduler\n",
    "    scheduler.step()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---> after epoch 0 the macro-f1 on dev set is 0.45486502427294584\n",
      "learning rate 0.001\n",
      "best model updated; new best f1 0.45486502427294584\n",
      "\n",
      "---> after epoch 1 the macro-f1 on dev set is 0.4644324053344271\n",
      "learning rate 0.001\n",
      "best model updated; new best f1 0.4644324053344271\n",
      "\n",
      "---> after epoch 2 the macro-f1 on dev set is 0.46333100967008783\n",
      "learning rate 0.001\n",
      "\n",
      "---> after epoch 3 the macro-f1 on dev set is 0.521627206856991\n",
      "learning rate 0.001\n",
      "best model updated; new best f1 0.521627206856991\n",
      "\n",
      "---> after epoch 4 the macro-f1 on dev set is 0.5512225881481505\n",
      "learning rate 0.001\n",
      "best model updated; new best f1 0.5512225881481505\n",
      "\n",
      "---> after epoch 5 the macro-f1 on dev set is 0.5691037558567813\n",
      "learning rate 0.001\n",
      "best model updated; new best f1 0.5691037558567813\n",
      "\n",
      "---> after epoch 6 the macro-f1 on dev set is 0.5690961164035612\n",
      "learning rate 0.001\n",
      "\n",
      "---> after epoch 7 the macro-f1 on dev set is 0.5941469071378437\n",
      "learning rate 0.001\n",
      "best model updated; new best f1 0.5941469071378437\n",
      "\n",
      "---> after epoch 8 the macro-f1 on dev set is 0.5870082198025167\n",
      "learning rate 0.001\n",
      "\n",
      "---> after epoch 9 the macro-f1 on dev set is 0.5880314945336153\n",
      "learning rate 0.001\n",
      "\n",
      "---> after epoch 10 the macro-f1 on dev set is 0.5911312245400082\n",
      "learning rate 0.0009000000000000001\n",
      "\n",
      "---> after epoch 11 the macro-f1 on dev set is 0.605582569103889\n",
      "learning rate 0.0009000000000000001\n",
      "best model updated; new best f1 0.605582569103889\n",
      "\n",
      "---> after epoch 12 the macro-f1 on dev set is 0.6068825264701188\n",
      "learning rate 0.0009000000000000001\n",
      "best model updated; new best f1 0.6068825264701188\n",
      "\n",
      "---> after epoch 13 the macro-f1 on dev set is 0.6177989036477942\n",
      "learning rate 0.0009000000000000001\n",
      "best model updated; new best f1 0.6177989036477942\n",
      "\n",
      "---> after epoch 14 the macro-f1 on dev set is 0.6071027291828182\n",
      "learning rate 0.0009000000000000001\n",
      "\n",
      "---> after epoch 15 the macro-f1 on dev set is 0.5928059285956055\n",
      "learning rate 0.0009000000000000001\n",
      "\n",
      "---> after epoch 16 the macro-f1 on dev set is 0.6113820183239924\n",
      "learning rate 0.0009000000000000001\n",
      "\n",
      "---> after epoch 17 the macro-f1 on dev set is 0.6188764084260621\n",
      "learning rate 0.0009000000000000001\n",
      "best model updated; new best f1 0.6188764084260621\n",
      "\n",
      "---> after epoch 18 the macro-f1 on dev set is 0.646016601710786\n",
      "learning rate 0.0009000000000000001\n",
      "best model updated; new best f1 0.646016601710786\n",
      "\n",
      "---> after epoch 19 the macro-f1 on dev set is 0.6194270096072267\n",
      "learning rate 0.0009000000000000001\n",
      "\n",
      "---> after epoch 20 the macro-f1 on dev set is 0.6167170163377278\n",
      "learning rate 0.0008100000000000001\n",
      "\n",
      "---> after epoch 21 the macro-f1 on dev set is 0.6323796887525224\n",
      "learning rate 0.0008100000000000001\n",
      "\n",
      "---> after epoch 22 the macro-f1 on dev set is 0.6391152636973286\n",
      "learning rate 0.0008100000000000001\n",
      "\n",
      "---> after epoch 23 the macro-f1 on dev set is 0.64785948067243\n",
      "learning rate 0.0008100000000000001\n",
      "best model updated; new best f1 0.64785948067243\n",
      "\n",
      "---> after epoch 24 the macro-f1 on dev set is 0.6440487080165277\n",
      "learning rate 0.0008100000000000001\n",
      "\n",
      "---> after epoch 25 the macro-f1 on dev set is 0.6323416355898368\n",
      "learning rate 0.0008100000000000001\n",
      "\n",
      "---> after epoch 26 the macro-f1 on dev set is 0.644106827962208\n",
      "learning rate 0.0008100000000000001\n",
      "\n",
      "---> after epoch 27 the macro-f1 on dev set is 0.6404272747718471\n",
      "learning rate 0.0008100000000000001\n",
      "\n",
      "---> after epoch 28 the macro-f1 on dev set is 0.6438641413235344\n",
      "learning rate 0.0008100000000000001\n",
      "\n",
      "---> after epoch 29 the macro-f1 on dev set is 0.648587333682332\n",
      "learning rate 0.0008100000000000001\n",
      "best model updated; new best f1 0.648587333682332\n",
      "\n",
      "---> after epoch 30 the macro-f1 on dev set is 0.6507496943716119\n",
      "learning rate 0.000729\n",
      "best model updated; new best f1 0.6507496943716119\n",
      "\n",
      "---> after epoch 31 the macro-f1 on dev set is 0.6508582623613127\n",
      "learning rate 0.000729\n",
      "best model updated; new best f1 0.6508582623613127\n",
      "\n",
      "---> after epoch 32 the macro-f1 on dev set is 0.654637014917554\n",
      "learning rate 0.000729\n",
      "best model updated; new best f1 0.654637014917554\n",
      "\n",
      "---> after epoch 33 the macro-f1 on dev set is 0.6618098051703749\n",
      "learning rate 0.000729\n",
      "best model updated; new best f1 0.6618098051703749\n",
      "\n",
      "---> after epoch 34 the macro-f1 on dev set is 0.6460812896587207\n",
      "learning rate 0.000729\n",
      "\n",
      "---> after epoch 35 the macro-f1 on dev set is 0.6470771347408222\n",
      "learning rate 0.000729\n",
      "\n",
      "---> after epoch 36 the macro-f1 on dev set is 0.6609505703873271\n",
      "learning rate 0.000729\n",
      "\n",
      "---> after epoch 37 the macro-f1 on dev set is 0.647106575722889\n",
      "learning rate 0.000729\n",
      "\n",
      "---> after epoch 38 the macro-f1 on dev set is 0.6447409192632123\n",
      "learning rate 0.000729\n",
      "\n",
      "---> after epoch 39 the macro-f1 on dev set is 0.6576675342521587\n",
      "learning rate 0.000729\n",
      "\n",
      "---> after epoch 40 the macro-f1 on dev set is 0.6529547371587937\n",
      "learning rate 0.0006561000000000001\n",
      "\n",
      "---> after epoch 41 the macro-f1 on dev set is 0.6612552011876693\n",
      "learning rate 0.0006561000000000001\n",
      "\n",
      "---> after epoch 42 the macro-f1 on dev set is 0.6460271897056953\n",
      "learning rate 0.0006561000000000001\n",
      "\n",
      "---> after epoch 43 the macro-f1 on dev set is 0.6483993091928315\n",
      "learning rate 0.0006561000000000001\n",
      "\n",
      "---> after epoch 44 the macro-f1 on dev set is 0.6505101049483077\n",
      "learning rate 0.0006561000000000001\n",
      "\n",
      "---> after epoch 45 the macro-f1 on dev set is 0.6482302453449941\n",
      "learning rate 0.0006561000000000001\n",
      "\n",
      "---> after epoch 46 the macro-f1 on dev set is 0.6534622439001812\n",
      "learning rate 0.0006561000000000001\n",
      "\n",
      "---> after epoch 47 the macro-f1 on dev set is 0.6446542441125521\n",
      "learning rate 0.0006561000000000001\n",
      "\n",
      "---> after epoch 48 the macro-f1 on dev set is 0.6508282835797473\n",
      "learning rate 0.0006561000000000001\n",
      "\n",
      "---> after epoch 49 the macro-f1 on dev set is 0.6413172531753012\n",
      "learning rate 0.0006561000000000001\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "ExecuteTime": {
     "end_time": "2024-12-09T07:48:47.444459Z",
     "start_time": "2024-12-09T07:48:47.098137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# test on the test set\n",
    "\n",
    "# load the best model weights\n",
    "model.load_state_dict(best_model)\n",
    "test_vecs = np.array([vectorize_sent(word_vectors, ss) for ss in test_docs])\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    test_data = torch.tensor(test_vecs, dtype=torch.float)\n",
    "    test_target = torch.tensor(test_labels, dtype=torch.int64)\n",
    "    test_prediction = model(test_data)\n",
    "    pred_labels = [np.argmax(dp.numpy()) for dp in test_prediction]\n",
    "    pre, rec, f1, _ = precision_recall_fscore_support(test_target, pred_labels, average='macro')\n",
    "    print('macro-f1 on test data', f1)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro-f1 on test data 0.6744302398301264\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## SAVE YOUR TRAINED MODEL"
  },
  {
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    },
    "ExecuteTime": {
     "end_time": "2024-12-09T07:48:47.474286Z",
     "start_time": "2024-12-09T07:48:47.470667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "\n",
    "# save model and other necessary components of your model\n",
    "# DO NOT include the embedding files in your submission\n",
    "\n",
    "all_info_want_to_save = {\n",
    "    'input_dim': word_vec_dim,\n",
    "    'dropout_rate': dropout_rate,\n",
    "    'neural_weights': best_model,\n",
    "    'oov_vector': oov_vec\n",
    "}\n",
    "save_path = open(\"propoganda.pickle\",\"wb\")\n",
    "pickle.dump(all_info_want_to_save, save_path)\n",
    "save_path.close()"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T07:48:47.502067Z",
     "start_time": "2024-12-09T07:48:47.500563Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
